# -*- coding: utf-8 -*-
"""rbm_ansatz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebook#fileId=1LHvyDpfj5GwooqyDKi02hj1xchemPWko
"""

"""# Train an RBM to approximate a quantum state
With RBM as a state ansatz, we need to train its parameters in order to make it have maximum overlap with the ground state wave function of a hamiltonian.

Notice that the cost function now is the energy rather than the log-likelihood with some known dataset:
$$E_\theta=\frac{\langle\psi_\theta|H|\psi_\theta\rangle}{\langle\psi_\theta|\psi_\theta\rangle}$$
Here, $\theta$ represents parameters in a RBM.

We use Variational Monte Carlo (VMC) method to get the energy expectation and gradients to optimize the ansatz.
VMC states that the gradient of $E$ with respect to $\theta$ is defined as 
$$g=\langle E_{\rm loc}\Delta_{\rm loc}\rangle- \langle E_{\rm loc}\rangle\langle\Delta_{\rm loc}\rangle$$
where $\langle\cdot\rangle$ is the ensemble average over Monte Carlo importance sampling, $E_{\rm loc}=\frac{\langle\psi|H|\sigma\rangle}{\langle\sigma|\psi\rangle}$, $\Delta_{\rm loc}\equiv\frac{\partial\log\psi}{\partial \theta}$.
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

"""## Wave function ansatz
Comparing with the model defined in last example, we make serveral adaptions here
* using {-1, 1} valued nodes instead of {0, 1} valued nodes.
* use probability function of visual nodes directly, $p(v)\sim e^{x^T a}\prod\limits_i\cosh(W_ix+b_i)$.
"""

class RBM(nn.Module):
    '''
    Restricted Boltzmann Machine

    Args:
        num_visible (int): number of visible nodes.
        num_hidden (int): number of hidden nodes.

    Attributes:
        W (2darray): weights.
        v_bias (1darray): bias for visible layer.
        h_bias (1darray): bias for hidden layer.
    '''

    def __init__(self, num_visible, num_hidden):
        super(RBM, self).__init__()
        self.W = nn.Parameter(torch.randn(num_hidden, num_visible).double() * 3e-2)
        self.v_bias = nn.Parameter(torch.zeros(num_visible).double())
        self.h_bias = nn.Parameter(torch.randn(num_hidden).double() * 3e-2)
        self.num_visible = num_visible
        self.num_hidden = num_hidden
        
    def prob_visible(self, v):
        '''
        probability for visible nodes, visible/hidden nodes here take value in {-1, 1}.

        Args:
            v (1darray): visible input.

        Return:
            float: the probability of v.
        '''
        v = Variable(v.double())
        if self.W.is_cuda: v = v.cuda()
        return (2 * (self.W.mv(v) + self.h_bias).cosh()).prod() * (self.v_bias.dot(v)).exp()

"""## The training process"""

def train(model, learning_rate, use_cuda):
    '''
    train a model.

    Args:
        model (obj): a model that meet VMC model definition.
        learning_rate (float): the learning rate for SGD.
    '''
    num_spin = model.ansatz.num_visible
    initial_config = np.array([-1, 1] * (num_spin // 2))
    if use_cuda:
        model.ansatz.cuda()

    while True:
        # get expectation values for energy, gradient and their product,
        # as well as the precision of energy.
        sample_list = vmc_sample(model, initial_config, num_bath=200, num_sample=4000)
        energy, grad, energy_grad, precision = vmc_measure(sample_list, num_spin)

        # update variables using steepest gradient descent
        g_list = [eg - energy * g for eg, g in zip(energy_grad, grad)]
        for var, g in zip(model.ansatz.parameters(), g_list):
            delta = learning_rate * g
            var.data -= delta
        yield energy, precision

"""## VMC measurements
VMC gives expectation values for $\langle E_{\rm loc}\rangle$, $\langle\Delta_{\rm loc}\rangle$ and $\langle E_{\rm loc}\Delta_{\rm loc}\rangle$.
It requires a kernel with following methods
* propose a new configuration, given old configuration.
* give probability for a spin configuration $\langle\sigma|\psi\rangle$.
* provide local quantities $E_{\rm loc}$ and $\Delta_{\rm loc}$ for given spin configuration.
"""

def vmc_sample(model, initial_config, num_bath, num_sample):
    '''
    obtain a set of samples.

    Args:
        model (Model): model definition, requiring the following methods:
            * local_measure, get local energy and local gradient.
            * prob, get the probability of specific distribution.
        initial_config (1darray): initial configuration.
        num_bath (int): number of updates to thermalize.
        num_sample (int): number of samples.

    Return:
        list: a list of spin configurations.
    '''
    print_step = num_sample // 5

    config = initial_config
    prob = model.prob(config)

    n_accepted = 0
    sample_list = []
    for i in range(num_bath + num_sample):
        # generate new config and calculate probability ratio
        config_proposed = model.propose_config(config)
        prob_proposed = model.prob(config_proposed)

        # accept/reject a move by metropolis algorithm (world's most famous single line algorithm)
        if np.random.random() < prob_proposed / prob:
            config = config_proposed
            prob = prob_proposed
            n_accepted += 1

        # print statistics
        if i % print_step == print_step - 1:
            print('%-10s Accept rate: %.3f' %
                  (i + 1, n_accepted * 1. / print_step))
            n_accepted = 0

        # add a sample
        if i >= num_bath:
            sample_list.append(config_proposed)
    return sample_list


def vmc_measure(sample_list, measure_step, num_bin=50):
    '''
    perform measurements on samples

    Args:
        sample_list (list): a list of spin configurations.
        num_bin (int): number of bins in binning statistics.
        meaure_step: number of samples skiped between two measurements + 1.

    Returns:
        tuple: expectation valued of energy, gradient, energy*gradient and error of energy.
    '''
    # measurements
    energy_loc_list, grad_loc_list = [], []
    for i, config in enumerate(sample_list):
        if i % measure_step == 0:
            # here, I choose a lazy way that re-compute on this config, in order to get its gradients easily.
            energy_loc, grad_loc = model.local_measure(config)
            energy_loc_list.append(energy_loc)
            grad_loc_list.append(grad_loc)

    # binning statistics
    energy_loc_list = np.array(energy_loc_list)
    energy, energy_precision = binning_statistics(energy_loc_list, num_bin=num_bin)

    # get sample expectations
    energy_loc_list = torch.from_numpy(energy_loc_list)
    if grad_loc_list[0][0].is_cuda: energy_loc_list = energy_loc_list.cuda()
    grad_mean = []
    energy_grad = []
    for grad_loc in zip(*grad_loc_list):
        grad_loc = torch.stack(grad_loc, 0)
        grad_mean.append(grad_loc.mean(0))
        energy_grad.append(
            (energy_loc_list[(slice(None),) + (None,) * (grad_loc.dim() - 1)] * grad_loc).mean(0))
    return energy.item(), grad_mean, energy_grad, energy_precision


def binning_statistics(var_list, num_bin):
    '''
    binning statistics for variable list.
    '''
    num_sample = len(var_list)
    if num_sample % num_bin != 0:
        raise ValueError('number of bin %d can not devide the number of data %d.'%(num_bin, len(var_list)))
    size_bin = num_sample // num_bin

    # mean, variance
    mean = np.mean(var_list, axis=0)
    variance = np.var(var_list, axis=0)

    # binned variance and autocorrelation time.
    variance_binned = np.var(
        [np.mean(var_list[size_bin * i:size_bin * (i + 1)]) for i in range(num_bin)])
    t_auto = 0.5 * size_bin * \
        np.abs(np.mean(variance_binned) / np.mean(variance))
    stderr = np.sqrt(variance_binned / num_bin)
    print('Binning Statistics: Energy = %.4f +- %.4f, Auto correlation Time = %.4f' %
          (mean, stderr, t_auto))
    return mean, stderr

"""### VMC Kernel definition
When proposing a new configuration, it has 5% probability to flip all spin, can making VMC sample better in Heisenberg model, proposed configuration satisfies spin conservation.
"""

class VMCKernel(object):
    '''
    variational monte carlo kernel.

    Attributes:
        energy_loc (func): local energy <\sigma|H|\psi>/<\sigma|\psi>.
        ansatz (Module): torch neural network.
    '''
    def __init__(self, energy_loc, ansatz):
        self.ansatz = ansatz
        self.energy_loc = energy_loc

    def psi(self, config):
        '''
        query the wavefunction.

        Args:
            config (1darray): the bit string as a configuration.

        Returns:
            Variable: the projection of wave function on config, i.e. <config|psi>.
        '''
        psi = self.ansatz.prob_visible(torch.from_numpy(config))
        return psi

    def prob(self, config):
        '''
        probability of configuration.

        Args:
            config (1darray): the bit string as a configuration.

        Returns:
            number: probability |<config|psi>|^2.
        '''
        return abs(self.psi(config).data[0])**2

    def local_measure(self, config):
        '''
        get local quantities energy_loc, grad_loc.

        Args:
            config (1darray): the bit string as a configuration.

        Returns:
            number, list: local energy and local gradients for variables.
        '''
        psi_loc = self.psi(config)

        # get gradients {d/dW}_{loc}
        self.ansatz.zero_grad()
        psi_loc.backward()
        grad_loc = [p.grad.data/psi_loc.data[0] for p in self.ansatz.parameters()]

        # E_{loc}
        eloc = self.energy_loc(config, lambda x: self.psi(x).data, psi_loc.data)[0]
        return eloc, grad_loc

    @staticmethod
    def propose_config(old_config, prob_flip=0.05):
        '''
        flip two positions as suggested spin flips.

        Args:
            old_config (1darray): spin configuration, which is a [-1,1] string.
            prob_flip (float): the probability to flip all spins, to make VMC more statble in Heisenberg model.

        Returns:
            1darray: new spin configuration.
        '''
        # take ~ 5% probability to flip all spin, can making VMC sample better in Heisenberg model
        if np.random.random() < prob_flip:
            return -old_config

        num_spin = len(old_config)
        upmask = old_config == 1
        flips = np.random.randint(0, num_spin // 2, 2)
        iflip0 = np.where(upmask)[0][flips[0]]
        iflip1 = np.where(~upmask)[0][flips[1]]

        config = old_config.copy()
        config[iflip0] = -1
        config[iflip1] = 1
        return config

"""## Hamiltonian
Here, we use the Heisenberg model with rotated basis
$$H=\sum\limits_i J_z S_i^zS_{i+1}^z-J_{xy}(S^+_{i}S^-_{i+1}+S^-_{i}S^+_{i+1})$$
Comparing with traditional Heisenberg model, we performed transformation $S^{x,y}_i=-S^{x,y}_i,i\in \rm odd$, intended to make the ground state wave function always positive.

Instead of defining the hamiltonian directlt, we define a function to calculate $E_{\rm loc}$
"""

def heisenberg_loc(config, psi_func, psi_loc, J=1.):
    '''
    local energy for 1D Periodic Heisenberg chain.

    Args:
        config (1darray): bit string as spin configuration.
        psi_func (func): wave function.
        psi_loc (number): wave function projected on configuration <config|psi>.
        J (float): coupling strengh. Here, J = Jxy = Jz.

    Returns:
        number: local energy.
    '''
    # get weights and flips after applying hamiltonian \sum_i w_i|sigma_i> = H|sigma>
    nsite = len(config)
    wl, flips = [], []
    # J*SzSz terms.
    nn_par = np.roll(config, -1) * config
    wl.append(J / 4. * (nn_par).sum(axis=-1).item())
    flips.append(np.array([], dtype='int64'))

    # J*SxSx and J*SySy terms.
    mask = nn_par != 1
    i = np.where(mask)[0]
    j = (i + 1) % nsite
    wl += [-J / 2.] * len(i)
    flips.extend(zip(i, j))

    # calculate local energy <psi|H|sigma>/<psi|sigma>
    acc = 0.
    for wi, flip in zip(wl, flips):
        config_i = config.copy()
        config_i[list(flip)] *= -1
        eng_i = wi * (psi_func(config_i) / psi_loc)
        acc += eng_i
    return acc

"""## Start training
We will see GPU is not better than CPU in this case. There are mainly two factors causing this
* the scale of vectorized operation is not large enough
* freqent data transfer between CPU and GPU can become a bottleneck.
"""

# %matplotlib inline
import time
import matplotlib.pyplot as plt

# set random number seed
seed = 10086
torch.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
np.random.seed(seed)
max_iter = 100
num_spin = 8
num_hidden = 16
E_exact = -3.65109341

# visualize the loss history
energy_list, precision_list = [], []
def _update_curve(energy, precision):
    energy_list.append(energy)
    precision_list.append(precision)
    plt.errorbar(np.arange(1, len(energy_list) + 1), energy_list, yerr=precision_list, capsize=3)
    plt.axhline(E_exact, ls='--')
    plt.show()

rbm = RBM(num_spin, num_hidden)
model = VMCKernel(heisenberg_loc, ansatz=rbm)

t0 = time.time()
for i, (energy, precision) in enumerate(train(model, learning_rate = 0.1, use_cuda = False)):
    t1 = time.time()
    print('Step %d, dE/|E| = %.4f, elapse = %.4f' % (i, -(energy - E_exact)/E_exact, t1-t0))
    _update_curve(energy, precision)
    t0 = time.time()

    # stop condition
    if i >= max_iter:
        break

"""## Challenge
J1-J2 model is an advanced version of Heisenberg model that having 2nd nearest neighbor coupling terms. Its hamiltonian is 

$$H=J_1\sum\limits_i \vec S_i\cdot \vec S_{i+1}+ J_2\sum_i\vec S_i\cdot \vec S_{i+2}$$

The sign of its ground state wave functions $\psi(x)=\langle x|\psi\rangle$ can be negative. Real valued RBM is not able to describe such a state.

With the following local energy function, write your own ansatz to obtain its ground state.

As a start point, you can try with 20 sites, $J_2=0.2$. And here is a table of DMRG results for benchmark, which is believe to be "exact" in 1D systems. Get the lowest energy as you can.

| &nbsp;&nbsp;&nbsp;$J_2$ &nbsp;&nbsp;&nbsp;  | 20             | 30             | 40             | 100            |
| -------------------- | -------------- | -------------- | -------------- | -------------- |
| 0.                   | -8.90438652988&nbsp;&nbsp;&nbsp; | -13.3219630586&nbsp;&nbsp;&nbsp; | -17.7465227719&nbsp;&nbsp;&nbsp; | -44.3229467082&nbsp;&nbsp;&nbsp; |
| 0.2                  | -8.20291625218 | -12.2770471706 | -16.3566615295 | -40.8572924302 |
| 0.5                  | -7.5           | -11.25         | -15            | 37.5           |
| 0.8                  | -8.46127240196 | -12.6588455544 | -16.8706800952 | -42.0700632095 |
"""

def J1J2_loc(config, psi_func, psi_loc, J1, J1z, J2, J2z, periodic):
    '''
    local energy for 1D J1, J2 chain.

    Args:
        config (1darray): bit string as spin configuration.
        psi_func (func): wave function.
        psi_loc (number): wave function projected on configuration <config|psi>.
        J1, J1z, J2, J2z (float): coupling strenghs for nearest neightbor, nearest neighbor z-direction, 2nd nearest neighbor and 2nd nearest neighbor in z-direction.
        periodic (bool): boundary condition.

    Returns:
        number: local energy.
    '''
    nsite = len(config)
    wl, flips = [], []
    Js = [J1, J2]
    Jzs = [J1z, J2z]
    for INB, (J,Jz) in enumerate(zip(Js, Jzs)):
        # J1(SzSz) terms.
        nn_par = np.roll(config, -INB-1) * config
        if not periodic:
            nn_par = nn_par[:-INB-1]
        wl.append(Jz / 4. * (nn_par).sum(axis=-1))
        flips.append(np.array([], dtype='int64'))

        # J1(SxSx) and J1(SySy) terms.
        mask = nn_par != 1
        i = np.where(mask)[0]
        j = (i + INB+1) % nsite

        wl += [J / 2.] * len(i)
        flips.extend(zip(i, j))
        
    # calculate local energy <psi|H|sigma>/<psi|sigma>
    acc = 0
    for wi, flip in zip(wl, flips):
        config_i = config.copy()
        config_i[list(flip)] *= -1
        eng_i = wi * psi_func(config_i) / psi_loc
        acc += eng_i
    return acc

"""## Ways to improve
This code is motivated for tutorial, there are many aspects to improve.
* using **symmetries** in your model, translation ,spin flip and inversion to make training more stable.
* stochastic reconfiguration or Adam as a better **optimization** method.
* rewrite the part of code to get local energy using C/C++/Fortran.
* use tensorflow or your own code to support **complex functions**.
"""

