{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rbm_generation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1WBu2FzHkYE862gZMXq-DgP5gdO0FpJ5x",
          "timestamp": 1519110984224
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "u0Bp-Metx3XM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Restricted Boltzmann Machine as a Generative Model\n",
        "The following code is partly referenced from\n",
        "https://github.com/odie2630463/Restricted-Boltzmann-Machines-in-pytorch\n",
        "## Setup PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "AVQfrTWR-43O",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "    !pip install torchvision\n",
        "    import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "99w9A3nOfcVI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### make sure we are using GPU (Nvidia K80)"
      ]
    },
    {
      "metadata": {
        "id": "L6uY7QZV_QHh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6abe4fb6-bad5-4237-f0f2-804b90d6e093",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518832668359,
          "user_tz": -480,
          "elapsed": 579,
          "user": {
            "displayName": "刘金国",
            "photoUrl": "//lh3.googleusercontent.com/-lDAT81T3HSE/AAAAAAAAAAI/AAAAAAAAAgw/eH3JEob7M1Y/s50-c-k-no/photo.jpg",
            "userId": "116824001998056121289"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "has_cuda = torch.cuda.is_available()\n",
        "if not has_cuda:\n",
        "    print('Not Using CUDA!')\n",
        "else:\n",
        "    print('Using CUDA device %d!'%torch.cuda.current_device())\n",
        "    ts = torch.Tensor([1,2,3])\n",
        "    ts = ts.cuda()\n",
        "    # you will see the location information of a tensor by printing it out.\n",
        "    print(ts)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Not Using CUDA!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sn9DboRcfkuf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Code an RBM\n",
        "#### Derivative of Negative log-likelihood\n",
        "$\\frac{\\partial{\\mathcal{L}}}{\\partial \\theta}=\\langle \\frac{\\partial E_\\theta(x)}{\\partial \\theta}\\rangle_{x\\sim\\mathcal{D}}-\\langle \\frac{\\partial E_\\theta(x)}{\\partial \\theta}\\rangle_{x\\sim p_{\\theta}(x)}$\n",
        "\n",
        "#### Free energy\n",
        "$E_\\theta(v) = -\\log(p_\\theta(x))$\n",
        "\n",
        "#### $k$-th order Contractive divergence (CD-$k$)\n",
        "Foward Gibbs sampling $x_0\\rightarrow h_0$ + backward Gibbs sampling $h_0\\rightarrow x_1$ + $\\ldots$ + backward Gibbs sampling $h_{k-1}\\rightarrow x_k$.\n",
        "\n",
        "For $k=\\infty$, we will get exact $p_{\\rm \\theta}(x)$."
      ]
    },
    {
      "metadata": {
        "id": "JcDupOzQ_ec6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class RBM(nn.Module):\n",
        "    '''\n",
        "    Restricted Boltzmann Machine\n",
        "\n",
        "    Args:\n",
        "        num_visible (int): number of visible nodes.\n",
        "        num_hidden (int): number of hidden nodes.\n",
        "\n",
        "    Attributes:\n",
        "        W (2darray): weights.\n",
        "        v_bias (1darray): bias for visible layer.\n",
        "        h_bias (1darray): bias for hidden layer.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, num_visible, num_hidden):\n",
        "        super(RBM, self).__init__()\n",
        "        self.W = nn.Parameter(torch.randn(num_hidden, num_visible) * 1e-2)\n",
        "        self.v_bias = nn.Parameter(torch.zeros(num_visible))\n",
        "        self.h_bias = nn.Parameter(torch.randn(num_hidden) * 1e-2)\n",
        "        self.num_visible = num_visible\n",
        "        self.num_hidden = num_hidden\n",
        "\n",
        "    def _v_to_h(self, v):\n",
        "        '''\n",
        "        forward pass p(h|v) from visible to hidden, v is visible input.\n",
        "        '''\n",
        "        p_h = F.sigmoid(F.linear(v, self.W, self.h_bias))\n",
        "        return p_h\n",
        "\n",
        "    def _h_to_v(self, h):\n",
        "        '''\n",
        "        backward pass p(v|h) from hidden to visible, h is hidden input.\n",
        "        '''\n",
        "        p_v = F.sigmoid(F.linear(h, self.W.t(), self.v_bias))\n",
        "        return p_v\n",
        "\n",
        "    def contrastive_divergence(self, v, k):\n",
        "        '''\n",
        "        Args:\n",
        "            v (ndarray): visible input.\n",
        "            k (in): CD-k, means k times v->h & h->v sweep in a single contrastive divergence run.\n",
        "\n",
        "        Returns:\n",
        "            ndarray: visible obtained through CD sampling.\n",
        "        '''\n",
        "        prob_h = self._v_to_h(v)\n",
        "        h = sample_from_prob(prob_h)\n",
        "        for _ in range(k):\n",
        "            prob_v = self._h_to_v(h)\n",
        "            v = sample_from_prob(prob_v)\n",
        "            prob_h = self._v_to_h(v)\n",
        "            h = sample_from_prob(prob_h)\n",
        "        return v\n",
        "\n",
        "    def free_energy(self, v):\n",
        "        '''\n",
        "        free energy E(x) = -log(\\sum_h exp(x, h)) = -log(p(x)*Z).\n",
        "        It can be used to obtain negative log-likelihood L = <E(x)>_{data} - <E(x)>_{model}.\n",
        "\n",
        "        Args:\n",
        "            v (1darray,2darray): visible input with size ([batch_size, ]data_size).\n",
        "\n",
        "        Return:\n",
        "            float: the free energy loss.\n",
        "        '''\n",
        "        vbias_term = v.mv(self.v_bias)\n",
        "        wx_b = F.linear(v, self.W, self.h_bias)\n",
        "        hidden_term = wx_b.exp().add(1).log().sum(dim=-1)\n",
        "        return (-hidden_term - vbias_term).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LEVdcofVU7Wx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def sample_from_prob(prob_list):\n",
        "    '''\n",
        "    from probability to 0-1 sample.\n",
        "\n",
        "    Args:\n",
        "        prob_list (1darray): probability of being 1.\n",
        "\n",
        "    Returns:\n",
        "        1darray: 0-1 array.\n",
        "    '''\n",
        "    rand = Variable(torch.rand(prob_list.size()))\n",
        "    if prob_list.is_cuda:\n",
        "        rand = rand.cuda()\n",
        "    return (1+torch.sign(prob_list - rand))/2.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6KHy9kQCfqvf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "metadata": {
        "id": "JugGWtAqVLfJ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def mnist01_loader(is_train, use_cuda, batch_size):\n",
        "    '''\n",
        "    yield image and label from mnist dataset.\n",
        "\n",
        "    Args:\n",
        "        is_train (bool): yield traning set if True, else test set.\n",
        "        use_cuda (bool): return data on GPU in True.\n",
        "        batch_size (int): size of a batch.\n",
        "\n",
        "    Returns:\n",
        "        func: an iterator function.\n",
        "    '''\n",
        "    from torchvision import datasets, transforms\n",
        "    import torch.utils.data\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('data', train=is_train,\n",
        "                    download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor()\n",
        "                       ])), batch_size=batch_size)\n",
        "\n",
        "    def iterator():\n",
        "        for data, label in test_loader:\n",
        "            # transform to binary mnist image\n",
        "            data = Variable(data.view(-1, 784))\n",
        "            data = data.bernoulli()\n",
        "            if use_cuda:\n",
        "                # copy data to gpu memory\n",
        "                data = data.cuda()\n",
        "                label = label.cuda()\n",
        "            yield data, label\n",
        "    return iterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1R8AUsRIVOia",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {},
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "5ef34f47-08c4-417d-c2ee-2635f2b054fe",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518832758648,
          "user_tz": -480,
          "elapsed": 917,
          "user": {
            "displayName": "刘金国",
            "photoUrl": "//lh3.googleusercontent.com/-lDAT81T3HSE/AAAAAAAAAAI/AAAAAAAAAgw/eH3JEob7M1Y/s50-c-k-no/photo.jpg",
            "userId": "116824001998056121289"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loader = mnist01_loader(is_train=True, use_cuda=False, batch_size=64)\n",
        "# let's check the data and labels\n",
        "for data, label in loader():\n",
        "    print(data.shape, label.shape)\n",
        "    print('A hand written digit with label \"%d\"'%label[0])\n",
        "    plt.imshow(data[0].data.numpy().reshape([28, 28]))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 784]) torch.Size([64])\n",
            "A hand written digit with label \"5\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABTVJREFUeJzt3cFqmEAYRlEtef9XttsQmuQqHXX0\nnHVJbaCXf/Mx67Zt2wLAj/5c/QEAMxBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEE\nCMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQ\nS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwB\nArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBgo+rPwBmtK5r/rPbtg38kt/t\n+dYRrv73/y8uS4BALAECsQQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQJzR4a7em53tVHTyLf/Xs/m\nsgQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAjEEiAwd5yUqds8Rr1u+JRXE2fhsgQIxBIgEEuA\nQCwBArEECMQSIBBLgEAsAQKxBAgseJiSh704m8sSIBBLgEAsAQKxBAjEEiAQS4BALAECsQQIxBIg\nEEuAwNxxUqPmfvXnjpoQjniEa8TPNKF8H5clQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngbnjC4yY+72d3+n7uCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4BALAECCx4OGfVgGtyVyxIgEEuA\nQCwBArEECMQSIBBLgEAsAQKxBAjEEiAQS4DA3JFbqdNID4ZxNpclQCCWAIFYAgRiCRCIJUAglgCB\nWAIEYgkQiCVAIJYAgbkjUxr1YqQZJd9xWQIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgTrZrLA\njYxa5lzJf7FncFkCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQmDsypSfOIpfFNPLOXJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE5o7wyUwzSv91z+WyBAjEEiAQS4BALAECsQQI\nxBIgEEuAQCwBArEECMQSIDB3hAPMIt/HZQkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRB8XP0B\ncMSoBc1Ma5eZvvUJXJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIE5o7wyUwPkXEulyVA\nIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBuSOH7JkF7nmF8IlzQ68wPoPLEiAQS4BALAEC\nsQQIxBIgEEuAQCwBArEECMQSILDgeYGrVzFX//17WNvwHZclQCCWAIFYAgRiCRCIJUAglgCBWAIE\nYgkQiCVAIJYAgbnjzdRp4NtneW//93M+lyVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCB\nueMXs7xEOMt3LotpIs/gsgQIxBIgEEuAQCwBArEECMQSIBBLgEAsAQKxBAgseL6oa5OZFjR7WNvA\nv7ksAQKxBAjEEiAQS4BALAECsQQIxBIgEEuAQCwBArEECMwdDzILhHdxWQIEYgkQiCVAIJYAgVgC\nBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCI\nJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYA\ngVgCBGIJEIglQCCWAIFYAgRiCRCIJUAglgCBWAIEYgkQiCVAIJYAgVgCBGIJEIglQPAX4ORwlehR\nIvUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f15f5c3c8d0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "U0gdtiqt34_k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use RBM as a Generative Model"
      ]
    },
    {
      "metadata": {
        "id": "gXQOWrPYVUrS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "0771d38e-10e4-4290-d9cc-da1bd31cba33",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518836669974,
          "user_tz": -480,
          "elapsed": 148086,
          "user": {
            "displayName": "刘金国",
            "photoUrl": "//lh3.googleusercontent.com/-lDAT81T3HSE/AAAAAAAAAAI/AAAAAAAAAgw/eH3JEob7M1Y/s50-c-k-no/photo.jpg",
            "userId": "116824001998056121289"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "use_cuda = True   # if you don't have cuda, this line must be set False!\n",
        "num_visible = 784\n",
        "num_hidden = 500\n",
        "\n",
        "# set seed for pytorch-cpu, pytorch-gpu and numpy\n",
        "seed = 10086\n",
        "torch.manual_seed(seed)\n",
        "if use_cuda: torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# define network and dataset, and transfer model data into GPU memory\n",
        "rbm = RBM(num_visible, num_hidden)\n",
        "if use_cuda: rbm = rbm.cuda()\n",
        "loader = mnist01_loader(True, use_cuda, batch_size=64)\n",
        "\n",
        "# with the stochastic gradient descent optimizer,\n",
        "# we optimize model parameters with learning rate 0.1.\n",
        "train_op = torch.optim.SGD(rbm.parameters(), 0.1)\n",
        "for epoch in range(20):\n",
        "    t0 = time.time()\n",
        "    loss_list = []\n",
        "    for data, label in loader():\n",
        "        # calculate the \"loss\", the last node in computation graph\n",
        "        v1 = rbm.contrastive_divergence(data, k=1)\n",
        "        loss = rbm.free_energy(data) - rbm.free_energy(v1)\n",
        "        loss_list.append(loss.data[0])\n",
        "\n",
        "        # get gradients using back propagation.\n",
        "        # zero_grad are needed before backward, otherwise gradients are accumulated.\n",
        "        train_op.zero_grad()\n",
        "        loss.backward()\n",
        "        # update parameters using gradients using gradients.\n",
        "        train_op.step()\n",
        "    t1 = time.time()\n",
        "    print('epoch %d, Mean \"Loss\" = %.4f, Elapse = %.4f'%(epoch, np.mean(loss_list), t1-t0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, Mean \"Loss\" = -8.4254, Elapse = 7.5652\n",
            "epoch 1, Mean \"Loss\" = -6.7092, Elapse = 7.3607\n",
            "epoch 2, Mean \"Loss\" = -4.6687, Elapse = 7.3725\n",
            "epoch 3, Mean \"Loss\" = -3.2327, Elapse = 7.3984\n",
            "epoch 4, Mean \"Loss\" = -2.2300, Elapse = 7.4317\n",
            "epoch 5, Mean \"Loss\" = -1.5764, Elapse = 7.3417\n",
            "epoch 6, Mean \"Loss\" = -1.0852, Elapse = 7.3662\n",
            "epoch 7, Mean \"Loss\" = -0.7575, Elapse = 7.3617\n",
            "epoch 8, Mean \"Loss\" = -0.4814, Elapse = 7.3095\n",
            "epoch 9, Mean \"Loss\" = -0.2918, Elapse = 7.3447\n",
            "epoch 10, Mean \"Loss\" = -0.0629, Elapse = 7.3505\n",
            "epoch 11, Mean \"Loss\" = 0.0438, Elapse = 7.3650\n",
            "epoch 12, Mean \"Loss\" = 0.2105, Elapse = 7.3781\n",
            "epoch 13, Mean \"Loss\" = 0.2326, Elapse = 7.3770\n",
            "epoch 14, Mean \"Loss\" = 0.2901, Elapse = 7.3454\n",
            "epoch 15, Mean \"Loss\" = 0.3734, Elapse = 7.3255\n",
            "epoch 16, Mean \"Loss\" = 0.4146, Elapse = 7.3898\n",
            "epoch 17, Mean \"Loss\" = 0.5676, Elapse = 7.3218\n",
            "epoch 18, Mean \"Loss\" = 0.6068, Elapse = 7.3093\n",
            "epoch 19, Mean \"Loss\" = 0.6057, Elapse = 7.3585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XDweacKvVzhE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "d1035948-87ef-4057-fc3d-e3154e98835c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518836670915,
          "user_tz": -480,
          "elapsed": 904,
          "user": {
            "displayName": "刘金国",
            "photoUrl": "//lh3.googleusercontent.com/-lDAT81T3HSE/AAAAAAAAAAI/AAAAAAAAAgw/eH3JEob7M1Y/s50-c-k-no/photo.jpg",
            "userId": "116824001998056121289"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# check for CD-1 generated data, they are similar to original data\n",
        "data, label = next(mnist01_loader(False, use_cuda, batch_size=32)())\n",
        "generated = rbm.contrastive_divergence(data, k=1)\n",
        "\n",
        "gs = plt.GridSpec(2,1)\n",
        "for irow, img in [(0, data), (1, generated)]:\n",
        "    img_grid = make_grid(img.view(32, 1, 28, 28).data)\n",
        "    npimg = np.transpose(img_grid.cpu().numpy(), (1, 2, 0))\n",
        "\n",
        "    plt.subplot(gs[irow,0])\n",
        "    plt.imshow(npimg)\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOoAAAD8CAYAAAB5GzjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQjVGAAvVGAH+Bg22Q0wVaOimAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4e2509eef0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fGVr6K_Y3ixs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Quiz\n",
        "* Try to explain why the loss goes up.\n",
        "* How much ($t_{\\rm CPU}/t_{\\rm GPU}$) a GPU can accelerate in this case?\n",
        "* Try CD-10 training, and CD-10 reconstruction, tell the difference."
      ]
    },
    {
      "metadata": {
        "id": "daD390CW2NKH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Challenge\n",
        "To see how RBM can be useful, try to solve the following problem.\n",
        "\n",
        "Top half of an image is lost, the only knowlege we know is it is a hand written number, try to recover this image.\n",
        "\n",
        "This image is:"
      ]
    },
    {
      "metadata": {
        "id": "JGHW_08wc7Lb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {}
          ],
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "b15d938e-befd-4f88-99f8-168e9772ba69",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1518836672153,
          "user_tz": -480,
          "elapsed": 961,
          "user": {
            "displayName": "刘金国",
            "photoUrl": "//lh3.googleusercontent.com/-lDAT81T3HSE/AAAAAAAAAAI/AAAAAAAAAgw/eH3JEob7M1Y/s50-c-k-no/photo.jpg",
            "userId": "116824001998056121289"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# presented data, npdata[:14] is replace by noises, with rest the correct values.\n",
        "npdata = np.array([0.87207395, 0.30172917, 0.20748128, 0.8095541, 0.6876969, 0.1532767, 0.62968737, 0.7779726, 0.7118719, 0.53654927, 0.94739664, 0.4471923, 0.8174037, 0.99884754, 0.88779926, 0.71121424, 0.3754819, 0.8892206, 0.6074483, 0.80546534, 0.86275977, 0.59773445, 0.8402653, 0.86507905, 0.42217958, 0.87019306, 0.7207597, 0.70173293, 0.5013676, 0.632447, 0.7610868, 0.9116219, 0.27281326, 0.5140466, 0.85905105, 0.6330603, 0.7861834, 0.48230517, 0.5583505, 0.46642596, 0.9083495, 0.5351944, 0.1025035, 0.098153934, 0.5750544, 0.16645348, 0.16748504, 0.88547695, 0.43294504, 0.6214652, 0.71871835, 0.091364935, 0.93197143, 0.18161148, 0.34041223, 0.36640248, 0.05945792, 0.84343576, 0.05011359, 0.020642783, 0.96630734, 0.02089095, 0.8930233, 0.3763136, 0.9165772, 0.97183466, 0.63145226, 0.9990819, 0.009387237, 0.50575066, 0.64946336, 0.35694385, 0.69572824, 0.9233393, 0.86297685, 0.95064604, 0.5617286, 0.20560175, 0.7403321, 0.27093577, 0.97413605, 0.26959816, 0.87772864, 0.044952888, 0.102199115, 0.9718467, 0.16127962, 0.655842, 0.15577097, 0.019478982, 0.79594487, 0.5539078, 0.23640345, 0.9437329, 0.9587844, 0.17794718, 0.6766039, 0.8290011, 0.4769842, 0.4305203, 0.8977303, 0.6218739, 0.7924648, 0.76474386, 0.3790167, 0.77901715, 0.14843856, 0.49938354, 0.0016661843, 0.53694785, 0.28133965, 0.50130546, 0.7144231, 0.21561551, 0.71205837, 0.22371855, 0.87220854, 0.26955116, 0.23340793, 0.35401136, 0.8505834, 0.3398249, 0.5540171, 0.10345899, 0.42512947, 0.038393788, 0.14067493, 0.35083002, 0.78112274, 0.7566516, 0.7144876, 0.85403043, 0.17873417, 0.35017425, 0.8706891, 0.9905999, 0.03664582, 0.21388373, 0.19397263, 0.49474058, 0.45710087, 0.5013515, 0.45443046, 0.49812528, 0.20585743, 0.6777932, 0.52848434, 0.24695043, 0.05329345, 0.8382803, 0.7743761, 0.75724375, 0.47649342, 0.3232994, 0.97870475, 0.95442826, 0.8724979, 0.7017745, 0.31166396, 0.86247295, 0.19751902, 0.7225639, 0.66581416, 0.053260047, 0.91416854, 0.5880709, 0.97473216, 0.11180539, 0.19542904, 0.6878177, 0.77867174, 0.6552874, 0.15082805, 0.036130577, 0.40634972, 0.92630273, 0.20962614, 0.34388322, 0.37616885, 0.15954784, 0.06997797, 0.53446907, 0.2530169, 0.15803383, 0.79036176, 0.84305835, 0.54108804, 0.8501002, 0.8879858, 0.75407153, 0.42112336, 0.9809118, 0.38506842, 0.70881325, 0.9115492, 0.98289156, 0.29546624, 0.5558135, 0.73718935, 0.4230012, 0.6937958, 0.16916722, 0.08621408, 0.6282674, 0.78306913, 0.032396235, 0.05875099, 0.08453581, 0.7941298, 0.44791576, 0.49781203, 0.24143413, 0.6512627, 0.20860681, 0.697907, 0.7528845, 0.372322, 0.7488793, 0.27171475, 0.72370976, 0.69339174, 0.8918535, 0.5999, 0.30938724, 0.3181647, 0.64025414, 0.78654414, 0.47598284, 0.8310249, 0.11720193, 0.2878025, 0.52041465, 0.9025964, 0.28781494, 0.8044487, 0.3056101, 0.33250284, 0.92022175, 0.286686, 0.9976094, 0.9832922, 0.31882682, 0.55324644, 0.43472946, 0.5232928, 0.54625267, 0.3906454, 0.62188476, 0.27731606, 0.16046757, 0.5718505, 0.35741845, 0.19788711, 0.6796674, 0.23651811, 0.08174702, 0.06336878, 0.09723609, 0.6873774, 0.14071466, 0.10673229, 0.72016853, 0.14792068, 0.6807766, 0.65582806, 0.30159968, 0.033888057, 0.9616834, 0.71672386, 0.7785365, 0.93777555, 0.4879574, 0.6355177, 0.5111196, 0.3009759, 0.3500596, 0.74764377, 0.13936538, 0.7483063, 0.81750077, 0.022937937, 0.4070562, 0.31818643, 0.5540159, 0.92213124, 0.73755354, 0.5571023, 0.75489575, 0.67721623, 0.72045094, 0.9948514, 0.70227563, 0.4672006, 0.106317885, 0.9804078, 0.6265774, 0.5751128, 0.6412867, 0.7626526, 0.63349885, 0.6859269, 0.42727026, 0.87387127, 0.30769062, 0.21811083, 0.7262418, 0.3143378, 0.5617967, 0.52690506, 0.5614177, 0.2285414, 0.85394025, 0.4490645, 0.3991946, 0.4290848, 0.260644, 0.45061842, 0.79720527, 0.8023241, 0.8577737, 0.15940316, 0.28310093, 0.5588658, 0.40793347, 0.3871484, 0.19407174, 0.090147026, 0.3032796, 0.56522983, 0.31781578, 0.9819852, 0.27190876, 0.27358428, 0.08329663, 0.9401394, 0.64813715, 0.43394515, 0.20502856, 0.6062421, 0.77016854, 0.6724567, 0.05586233, 0.30247933, 0.6605889, 0.59062487, 0.07923536, 0.79024094, 0.7947558, 0.47801945, 0.7789812, 0.7338063, 0.81782705, 0.86098266, 0.37724164, 0.025790595, 0.13603827, 0.70920366, 0.39994055, 0.9000277, 0.24124062, 0.556982, 0.9389691, 0.19500403, 0.0904483, 0.9657381, 0.36499324, 0.53046125, 0.7467004, 0.558991, 0.70214766, 0.047792256, 0.8731797, 0.6984725, 0.6879274, 0.28380287, 0.5657824, 0.82289946, 0.628091, 0.8842273, 0.6312523, 0.86718184, 0.046136532, 0.89170617, 0.27153188, 0.12342716, 0.4058729, 0.21863484, 0.7674653, 0.026953213, 0.44575384, 0.20404944, 0.8207133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], dtype='float32').reshape([28, 28])\n",
        "plt.imshow(npdata)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwYoOmCAogMGKDpg4G8CCHfrkCggAQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4e49f2f240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AbrkPDTZCNMK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, I will give you some **hint**.\n",
        "\n",
        "* Train a better RBM to complete this task.\n",
        "\n",
        "* Make this image a initial input, and perform Gibbs sampling sounds a good idea. Try to write a modified version of **contrastive_divergence** method.\n",
        "* data transform related API (pytorch API page: http://pytorch.org/docs/0.3.0/): \n",
        "\n",
        "```python\n",
        "    torch.from_numpy\n",
        "    torch.Tensor.numpy\n",
        "    torch.Tensor.cuda\n",
        "    torch.Tensor.cpu\n",
        "    torch.autograd.Variable\n",
        "```"
      ]
    }
  ]
}